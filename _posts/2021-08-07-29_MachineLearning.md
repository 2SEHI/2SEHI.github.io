---

title:  "[공정이상요인분석] 2021/8/7 Decision Tree, RandomForest에 대해 공부"
excerpt: "팀원 각자 분류 알고리즘을 분담하여 공부하고 모델 학습 및 훈련을 해보기로 했는데 저는 랜덤포레스트를 맡아 공부했습니다."

date: 2021-08-07
last_modified_at: 2021-08-07

use_math: true
comments: true

categories:
  - K-Digital Project
  - Machine Learning
---





## 랜덤 포레스트(RandomForest)

- 앙상블 : 랜덤 포레스트는 여러가지 분류기의 예측 결과를 결합하여 단일 분류기보다 신뢰성이 높은 예측값을 얻는 앙상블 학습 분류 기 입니다. 

- 앙상블 학습의 유형에는 보팅, 배깅, 부스팅의 세가지 방법이 있는데 랜덤포레스트 알고리즘은 그 중에서 각 분류기가 같은 유형의 알고리즘 기반인 배깅방식을 따릅니다.
  - 보팅(Voting) : 서로 다른 알고리즘을 가진 분류기를 결합하는 방식입니다.
  - 배깅(Bagging) : 같은 유형의 알고리즘을 결합하는 방식입니다.
  - 부스팅(Boosting) :여러 개의 분류기가 순차적으로 학습을 수행하되 앞에서 학습한 분류기가 예측이 틀린 데이터에 대해서는 올바르게 예측할 수 있도록 다음 분류기에게는 가중치를 부여하면서 학습과 예측을 진행하는 방식입니다.
- 랜덤 포레스트는 약한 학습기인 DecisionTree알고리즘이 서브트리로 존재하는 것입니다.

<br>

**랜덤 포레스트를 이해하기 위해 결정 트리(Decision Tree)에 대해 알아보겠습니다.**

<br>

## 결정 트리(Decision Tree)

- 일반적으로 if/else 규칙을 학습을 통해 찾아내 트리 기반의 분류 규칙을 만드는 알고리즘입니다.



### 결정 트리 모델의 구조

- 규칙 노드(Decision Node) : 규칙 조건이 되는 노드입니다
- 리프 노드(Leaf Node) : 결정된 클래스 값입니다.
- 서브 트리(Sub Tree)  규칙 노드 + 리프 노드
- 트리의 깊이(depth)

구조 그림 추가

<br>

- 노드(규칙)이 많다 ->  분류 결정 방식이 복잡하다 -> 과적합 가능성이 높다
  - 트리의 깊이가 깊어질수록 결정 트리의 예측 성능이 저하될 가능성이 높습니다.
- 이를 해결하기 위해서는 데이터 세트를 최대한 균일하게 구성하여 적은 노드(규칙)로 정확도를 높여야 합니다.
- 또한 피처수가 많을수록 많은 규칙이 필요해지므로 중요피처만 선출하거나, 하이퍼파라미터를 적절히 지정하여 복잡한 알고리즘을 만들지 않도록 해야 합니다.

<br>

### 결정 트리 모델의 특징

결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만듭니다. 

- *노란색 동그라미 사진 추가하기*



### 정보 균일도 측정 방법

정보의 균일도를 측정하는 대표적인 방법은 ```정보이득 지수```와 ```지니 계수```가 있습니다.

<br>

#### 정보 이득(Information Gain) 지수

정보 이득 지수는 엔트로피라는 개념을 이용하는데 엔트로피란 데이터 집합의 혼잡도를 의미합니다.  

$정보이득지수=1-엔트로피$

- 분류할 필요가 없을 경우엔 엔트로피가 1입니다. 일레이블이 불량품과 양품이 있고 L_A, L_B, L_C  센서의 값들에 따른 불량품과 양품의 분류기준에 다음과 같다고 가정해보겠습니다.





완벽하게 분류가 가능한 것은 과적합되었을 가능성이 있어 좋은 모델이라고 할 수는 없습니다. 학습데이터 이외의 데이터에 대해서는 좋은 성능을 나타내지 못할 수도 있기 때문입니다.	





#### 지니(Gini) 계수

엔트로피 이외에 불순도 지표로 많이 쓰이는 공식. 엔트로피보다 계산이 빠르기 때문에 주로 사용된다.



Decision Tree 알고리즘은 계산복잡성 대비 높은 예측 성능을 가지고 있다. 또한 변수 단위로 설명력을 지닌다는 강점을 가지고 있다. 다만 결정경계(decision boundary)가 데이터 축에 수직이어서 비선형(non-linear) 데이터 분류엔 적합하지 않는다.



## Feature Selection

- 모델의 정확도 혹은 속도를 높이기 위해 모든 피처를 사용하는 것이 아닌 학습에 필요한 적절한 피처만을 선택하는 과정을 뜻합니다.



### RFE

- 모든 피처들로부터 피처를 하나하나 제거하며 원하는 갯수의 피처가 남을 때까지 반복하는 방법입니다. 
- 모든 피처를 이용하여 모델을 학습할 때, 피처의 Feature Importance를 도출하여 Feature Importance가 가장 낮은 피처를 하나씩 제거해가며 원하는 피처의 갯수가 될 때까지 반복수행합니다.



### RFECV

- RFE의 단점은 몇 개의 피처를 남겨야 하는지 사용자가 지정해야 한다는 것인데, RFECV는 RFE와 같은 방식으로 피처를 제거해나가지만 RFE와 다른 점은 모델의 성능을 계산하며 가장 높은 성능을 가진 피처 개수가 될때까지 반복 수행한다는 것입니다.

